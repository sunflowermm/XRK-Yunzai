#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import asyncio
import re
import json
import base64
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# æ·»åŠ çˆ¶ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))

from lib.multiplugin.python_bridge import PythonPlugin, PluginRule

# é«˜çº§åŠŸèƒ½å¯¼å…¥
try:
    import numpy as np
    import pandas as pd
    HAS_DATA_LIBS = True
except ImportError:
    HAS_DATA_LIBS = False

try:
    from PIL import Image, ImageDraw, ImageFont
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')
    HAS_IMAGE_LIBS = True
except ImportError:
    HAS_IMAGE_LIBS = False

try:
    import aiohttp
    from bs4 import BeautifulSoup
    HAS_WEB_LIBS = True
except ImportError:
    HAS_WEB_LIBS = False

try:
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import StandardScaler
    HAS_ML_LIBS = True
except ImportError:
    HAS_ML_LIBS = False

try:
    import jieba
    import wordcloud
    HAS_NLP_LIBS = True
except ImportError:
    HAS_NLP_LIBS = False


class DataAnalysisPlugin(PythonPlugin):
    """æ•°æ®åˆ†ææ’ä»¶ - å±•ç¤ºPythonçš„æ•°æ®å¤„ç†èƒ½åŠ›"""
    
    def __init__(self):
        super().__init__()
        self.name = "Pythonæ•°æ®åˆ†æ"
        self.priority = 40
        
        self.rule = [
            PluginRule(
                reg=r"^#?pyåˆ†æ\s+(.+)$",
                fnc="analyze_data",
                event="message",
                log=True
            ),
            PluginRule(
                reg=r"^#?pyå›¾è¡¨\s+(.+)$",
                fnc="create_chart",
                event="message"
            ),
            PluginRule(
                reg=r"^#?pyé¢„æµ‹\s+(.+)$",
                fnc="predict",
                event="message"
            )
        ]
        
        # ç¼“å­˜æ•°æ®
        self.data_cache = {}
    
    async def analyze_data(self, *args):
        """æ•°æ®åˆ†æåŠŸèƒ½"""
        if not HAS_DATA_LIBS:
            return {"reply": "éœ€è¦å®‰è£…numpyå’Œpandasåº“æ‰èƒ½ä½¿ç”¨æ•°æ®åˆ†æåŠŸèƒ½"}
        
        e = self.e
        msg = e.get('msg', '')
        
        # æå–å‚æ•°
        match = re.search(r'pyåˆ†æ\s+(.+)$', msg)
        if not match:
            return {"reply": "è¯·æä¾›è¦åˆ†æçš„æ•°æ®ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰"}
        
        data_str = match.group(1).strip()
        
        try:
            # è§£ææ•°æ®
            data = [float(x.strip()) for x in data_str.split(',')]
            
            # ä½¿ç”¨numpyå’Œpandasè¿›è¡Œåˆ†æ
            arr = np.array(data)
            df = pd.DataFrame(data, columns=['value'])
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "æ•°æ®ç‚¹æ•°": len(data),
                "å¹³å‡å€¼": f"{np.mean(arr):.2f}",
                "ä¸­ä½æ•°": f"{np.median(arr):.2f}",
                "æ ‡å‡†å·®": f"{np.std(arr):.2f}",
                "æœ€å°å€¼": f"{np.min(arr):.2f}",
                "æœ€å¤§å€¼": f"{np.max(arr):.2f}",
                "25%åˆ†ä½": f"{df['value'].quantile(0.25):.2f}",
                "75%åˆ†ä½": f"{df['value'].quantile(0.75):.2f}",
                "å˜å¼‚ç³»æ•°": f"{(np.std(arr) / np.mean(arr) * 100):.2f}%"
            }
            
            # æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
            Q1 = df['value'].quantile(0.25)
            Q3 = df['value'].quantile(0.75)
            IQR = Q3 - Q1
            outliers = df[(df['value'] < Q1 - 1.5 * IQR) | (df['value'] > Q3 + 1.5 * IQR)]
            
            msg = "ğŸ“Š **æ•°æ®åˆ†æç»“æœ**\n"
            msg += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            for key, value in stats.items():
                msg += f"â–ª {key}: {value}\n"
            
            if not outliers.empty:
                msg += f"\nâš ï¸ æ£€æµ‹åˆ°{len(outliers)}ä¸ªå¼‚å¸¸å€¼:\n"
                msg += f"{outliers['value'].tolist()}"
            
            # ç¼“å­˜æ•°æ®ä¾›å›¾è¡¨ä½¿ç”¨
            self.data_cache[e.get('user_id')] = arr
            
            return {"reply": msg}
            
        except ValueError as e:
            return {"reply": f"æ•°æ®è§£æé”™è¯¯: {str(e)}"}
        except Exception as e:
            return {"reply": f"åˆ†æå¤±è´¥: {str(e)}"}
    
    async def create_chart(self, *args):
        """åˆ›å»ºæ•°æ®å›¾è¡¨"""
        if not HAS_IMAGE_LIBS or not HAS_DATA_LIBS:
            return {"reply": "éœ€è¦å®‰è£…matplotlibå’Œnumpyåº“æ‰èƒ½åˆ›å»ºå›¾è¡¨"}
        
        e = self.e
        msg = e.get('msg', '')
        user_id = e.get('user_id')
        
        # æå–å‚æ•°
        match = re.search(r'pyå›¾è¡¨\s+(.+)$', msg)
        chart_type = match.group(1).strip() if match else 'line'
        
        # æ£€æŸ¥ç¼“å­˜æ•°æ®
        if user_id not in self.data_cache:
            return {"reply": "è¯·å…ˆä½¿ç”¨ #pyåˆ†æ å‘½ä»¤åˆ†ææ•°æ®"}
        
        data = self.data_cache[user_id]
        
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(10, 6))
            
            if chart_type == 'hist' or chart_type == 'ç›´æ–¹å›¾':
                ax.hist(data, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
                ax.set_xlabel('å€¼')
                ax.set_ylabel('é¢‘æ•°')
                ax.set_title('æ•°æ®åˆ†å¸ƒç›´æ–¹å›¾')
            elif chart_type == 'box' or chart_type == 'ç®±çº¿å›¾':
                ax.boxplot(data, vert=True, patch_artist=True)
                ax.set_ylabel('å€¼')
                ax.set_title('ç®±çº¿å›¾åˆ†æ')
            elif chart_type == 'density' or chart_type == 'å¯†åº¦':
                from scipy import stats
                density = stats.gaussian_kde(data)
                x = np.linspace(data.min(), data.max(), 200)
                ax.plot(x, density(x), color='blue', linewidth=2)
                ax.fill_between(x, density(x), alpha=0.3, color='skyblue')
                ax.set_xlabel('å€¼')
                ax.set_ylabel('å¯†åº¦')
                ax.set_title('æ¦‚ç‡å¯†åº¦åˆ†å¸ƒ')
            else:  # é»˜è®¤æŠ˜çº¿å›¾
                ax.plot(range(len(data)), data, marker='o', linewidth=2, markersize=4)
                ax.set_xlabel('ç´¢å¼•')
                ax.set_ylabel('å€¼')
                ax.set_title('æ•°æ®è¶‹åŠ¿å›¾')
                ax.grid(True, alpha=0.3)
            
            # ä¿å­˜ä¸ºå›¾ç‰‡
            import io
            buf = io.BytesIO()
            plt.tight_layout()
            plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
            plt.close()
            
            # è½¬ä¸ºbase64
            buf.seek(0)
            img_base64 = base64.b64encode(buf.read()).decode()
            
            return {
                "reply": [
                    f"ğŸ“ˆ ç”Ÿæˆäº†{chart_type}å›¾è¡¨ï¼š",
                    {
                        "type": "image",
                        "file": f"base64://{img_base64}"
                    }
                ]
            }
            
        except Exception as e:
            return {"reply": f"åˆ›å»ºå›¾è¡¨å¤±è´¥: {str(e)}"}
    
    async def predict(self, *args):
        """æœºå™¨å­¦ä¹ é¢„æµ‹"""
        if not HAS_ML_LIBS or not HAS_DATA_LIBS:
            return {"reply": "éœ€è¦å®‰è£…scikit-learnåº“æ‰èƒ½ä½¿ç”¨é¢„æµ‹åŠŸèƒ½"}
        
        e = self.e
        msg = e.get('msg', '')
        
        # æå–å‚æ•°
        match = re.search(r'pyé¢„æµ‹\s+(.+)$', msg)
        if not match:
            return {"reply": "è¯·æä¾›å†å²æ•°æ®ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰"}
        
        data_str = match.group(1).strip()
        
        try:
            # è§£ææ•°æ®
            data = [float(x.strip()) for x in data_str.split(',')]
            
            if len(data) < 3:
                return {"reply": "éœ€è¦è‡³å°‘3ä¸ªæ•°æ®ç‚¹æ‰èƒ½è¿›è¡Œé¢„æµ‹"}
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X = np.array(range(len(data))).reshape(-1, 1)
            y = np.array(data)
            
            # æ ‡å‡†åŒ–
            scaler_X = StandardScaler()
            scaler_y = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)
            y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()
            
            # è®­ç»ƒæ¨¡å‹
            model = LinearRegression()
            model.fit(X_scaled, y_scaled)
            
            # é¢„æµ‹æœªæ¥5ä¸ªç‚¹
            future_points = 5
            future_X = np.array(range(len(data), len(data) + future_points)).reshape(-1, 1)
            future_X_scaled = scaler_X.transform(future_X)
            predictions_scaled = model.predict(future_X_scaled)
            predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).ravel()
            
            # è®¡ç®—æ¨¡å‹è¯„åˆ†
            score = model.score(X_scaled, y_scaled)
            
            # è®¡ç®—è¶‹åŠ¿
            slope = model.coef_[0]
            trend = "ä¸Šå‡" if slope > 0.1 else "ä¸‹é™" if slope < -0.1 else "å¹³ç¨³"
            
            msg = "ğŸ¤– **æœºå™¨å­¦ä¹ é¢„æµ‹ç»“æœ**\n"
            msg += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            msg += f"ğŸ“ˆ è¶‹åŠ¿: {trend}\n"
            msg += f"ğŸ“Š æ¨¡å‹å‡†ç¡®åº¦: {score*100:.2f}%\n"
            msg += f"\næœªæ¥{future_points}ä¸ªé¢„æµ‹å€¼:\n"
            
            for i, pred in enumerate(predictions, 1):
                msg += f"  {i}. {pred:.2f}\n"
            
            msg += f"\nğŸ’¡ æç¤º: åŸºäºçº¿æ€§å›å½’æ¨¡å‹"
            
            return {"reply": msg}
            
        except Exception as e:
            return {"reply": f"é¢„æµ‹å¤±è´¥: {str(e)}"}


class WebScraperPlugin(PythonPlugin):
    """ç½‘ç»œçˆ¬è™«æ’ä»¶ - å±•ç¤ºPythonçš„ç½‘ç»œæ•°æ®è·å–èƒ½åŠ›"""
    
    def __init__(self):
        super().__init__()
        self.name = "Pythonçˆ¬è™«"
        self.priority = 45
        
        self.rule = [
            PluginRule(
                reg=r"^#?pyçˆ¬å–\s+(.+)$",
                fnc="scrape_web",
                event="message"
            ),
            PluginRule(
                reg=r"^#?pyçƒ­æœ$",
                fnc="get_trending",
                event="message"
            )
        ]
    
    async def scrape_web(self, *args):
        """ç½‘é¡µçˆ¬å–"""
        if not HAS_WEB_LIBS:
            return {"reply": "éœ€è¦å®‰è£…aiohttpå’Œbeautifulsoup4åº“æ‰èƒ½ä½¿ç”¨çˆ¬è™«åŠŸèƒ½"}
        
        e = self.e
        msg = e.get('msg', '')
        
        match = re.search(r'pyçˆ¬å–\s+(.+)$', msg)
        if not match:
            return {"reply": "è¯·æä¾›è¦çˆ¬å–çš„URL"}
        
        url = match.group(1).strip()
        
        # éªŒè¯URL
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status != 200:
                        return {"reply": f"è·å–ç½‘é¡µå¤±è´¥: HTTP {response.status}"}
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # æå–ä¿¡æ¯
                    title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
                    
                    # æå–metaæè¿°
                    description = ""
                    meta_desc = soup.find('meta', attrs={'name': 'description'})
                    if meta_desc:
                        description = meta_desc.get('content', '')
                    
                    # ç»Ÿè®¡é“¾æ¥
                    links = soup.find_all('a')
                    images = soup.find_all('img')
                    
                    # æå–å‰5ä¸ªæ ‡é¢˜
                    headings = []
                    for tag in ['h1', 'h2', 'h3']:
                        for heading in soup.find_all(tag)[:2]:
                            text = heading.get_text(strip=True)
                            if text:
                                headings.append(f"[{tag.upper()}] {text}")
                    
                    msg = f"ğŸ•¸ï¸ **ç½‘é¡µä¿¡æ¯**\n"
                    msg += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    msg += f"ğŸ“Œ æ ‡é¢˜: {title}\n"
                    if description:
                        msg += f"ğŸ“ æè¿°: {description[:100]}...\n"
                    msg += f"ğŸ”— é“¾æ¥æ•°: {len(links)}\n"
                    msg += f"ğŸ–¼ï¸ å›¾ç‰‡æ•°: {len(images)}\n"
                    
                    if headings:
                        msg += f"\nğŸ“‘ ä¸»è¦æ ‡é¢˜:\n"
                        for h in headings[:5]:
                            msg += f"  â€¢ {h}\n"
                    
                    return {"reply": msg}
                    
        except asyncio.TimeoutError:
            return {"reply": "çˆ¬å–è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"}
        except Exception as e:
            return {"reply": f"çˆ¬å–å¤±è´¥: {str(e)}"}
    
    async def get_trending(self, *args):
        """è·å–çƒ­æœï¼ˆæ¨¡æ‹Ÿï¼‰"""
        if not HAS_WEB_LIBS:
            return {"reply": "éœ€è¦å®‰è£…ç½‘ç»œåº“æ‰èƒ½è·å–çƒ­æœ"}
        
        # è¿™é‡Œåº”è¯¥çˆ¬å–çœŸå®çš„çƒ­æœï¼Œä½†ä¸ºäº†ç¤ºä¾‹ç®€åŒ–
        import random
        
        topics = [
            "Python 3.12å‘å¸ƒæ–°ç‰¹æ€§",
            "AIç»˜ç”»æŠ€æœ¯çªç ´",
            "é‡å­è®¡ç®—æ–°è¿›å±•",
            "å…ƒå®‡å®™åº”ç”¨è½åœ°",
            "æ–°èƒ½æºæ±½è½¦é”€é‡åˆ›æ–°é«˜",
            "ChatGPTæ›´æ–°é‡å¤§åŠŸèƒ½",
            "åŒºå—é“¾æŠ€æœ¯æ–°åº”ç”¨",
            "äº‘è®¡ç®—å¸‚åœºç«äº‰åŠ å‰§",
            "5Gç½‘ç»œè¦†ç›–ç‡æå‡",
            "æœºå™¨å­¦ä¹ æ¡†æ¶æ›´æ–°"
        ]
        
        # éšæœºé€‰æ‹©å¹¶æ’åº
        selected = random.sample(topics, min(5, len(topics)))
        
        msg = "ğŸ”¥ **ä»Šæ—¥çƒ­æœ**\n"
        msg += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        
        for i, topic in enumerate(selected, 1):
            heat = random.randint(100000, 9999999)
            msg += f"{i}. {topic} ğŸ”¥{heat}\n"
        
        msg += f"\næ›´æ–°æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}"
        
        return {"reply": msg}


class NLPPlugin(PythonPlugin):
    """è‡ªç„¶è¯­è¨€å¤„ç†æ’ä»¶"""
    
    def __init__(self):
        super().__init__()
        self.name = "Python NLP"
        self.priority = 42
        
        self.rule = [
            PluginRule(
                reg=r"^#?pyåˆ†è¯\s+(.+)$",
                fnc="segment_text",
                event="message"
            ),
            PluginRule(
                reg=r"^#?pyè¯äº‘\s+(.+)$",
                fnc="word_cloud",
                event="message"
            ),
            PluginRule(
                reg=r"^#?pyæƒ…æ„Ÿ\s+(.+)$",
                fnc="sentiment_analysis",
                event="message"
            )
        ]
    
    async def segment_text(self, *args):
        """ä¸­æ–‡åˆ†è¯"""
        if not HAS_NLP_LIBS:
            return {"reply": "éœ€è¦å®‰è£…jiebaåº“æ‰èƒ½ä½¿ç”¨åˆ†è¯åŠŸèƒ½"}
        
        e = self.e
        msg = e.get('msg', '')
        
        match = re.search(r'pyåˆ†è¯\s+(.+)$', msg)
        if not match:
            return {"reply": "è¯·æä¾›è¦åˆ†è¯çš„æ–‡æœ¬"}
        
        text = match.group(1).strip()
        
        # æ‰§è¡Œåˆ†è¯
        words = jieba.lcut(text)
        
        # è¯é¢‘ç»Ÿè®¡
        from collections import Counter
        word_freq = Counter(words)
        
        # æå–å…³é”®è¯
        import jieba.analyse
        keywords = jieba.analyse.extract_tags(text, topK=5, withWeight=True)
        
        msg = "âœ‚ï¸ **åˆ†è¯ç»“æœ**\n"
        msg += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        msg += f"åˆ†è¯: {' / '.join(words)}\n\n"
        
        msg += "ğŸ“Š è¯é¢‘TOP5:\n"
        for word, freq in word_freq.most_common(5):
            msg += f"  â€¢ {word}: {freq}æ¬¡\n"
        
        msg += "\nğŸ”‘ å…³é”®è¯:\n"
        for word, weight in keywords:
            msg += f"  â€¢ {word}: {weight:.2f}\n"
        
        return {"reply": msg}
    
    async def word_cloud(self, *args):
        """ç”Ÿæˆè¯äº‘"""
        if not HAS_NLP_LIBS or not HAS_IMAGE_LIBS:
            return {"reply": "éœ€è¦å®‰è£…jiebaå’Œwordcloudåº“æ‰èƒ½ç”Ÿæˆè¯äº‘"}
        
        e = self.e
        msg = e.get('msg', '')
        
        match = re.search(r'pyè¯äº‘\s+(.+)$', msg)
        if not match:
            return {"reply": "è¯·æä¾›æ–‡æœ¬å†…å®¹"}
        
        text = match.group(1).strip()
        
        try:
            # åˆ†è¯
            words = ' '.join(jieba.cut(text))
            
            # ç”Ÿæˆè¯äº‘
            wc = wordcloud.WordCloud(
                width=800,
                height=400,
                background_color='white',
                font_path='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',  # éœ€è¦æ ¹æ®ç³»ç»Ÿè°ƒæ•´
                max_words=50
            ).generate(words)
            
            # ä¿å­˜ä¸ºå›¾ç‰‡
            import io
            from PIL import Image
            
            img = wc.to_image()
            buf = io.BytesIO()
            img.save(buf, format='PNG')
            
            # è½¬ä¸ºbase64
            buf.seek(0)
            img_base64 = base64.b64encode(buf.read()).decode()
            
            return {
                "reply": [
                    "â˜ï¸ ç”Ÿæˆäº†è¯äº‘å›¾ï¼š",
                    {
                        "type": "image",
                        "file": f"base64://{img_base64}"
                    }
                ]
            }
            
        except Exception as e:
            return {"reply": f"ç”Ÿæˆè¯äº‘å¤±è´¥: {str(e)}"}
    
    async def sentiment_analysis(self, *args):
        """æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        e = self.e
        msg = e.get('msg', '')
        
        match = re.search(r'pyæƒ…æ„Ÿ\s+(.+)$', msg)
        if not match:
            return {"reply": "è¯·æä¾›è¦åˆ†æçš„æ–‡æœ¬"}
        
        text = match.group(1).strip()
        
        # ç®€å•çš„æƒ…æ„Ÿè¯å…¸
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'å¼€å¿ƒ', 'å¿«ä¹', 'å¹¸ç¦', 'ç¾å¥½', 'èµ', 'çˆ±']
        negative_words = ['å·®', 'å', 'ç³Ÿç³•', 'è®¨åŒ', 'éš¾è¿‡', 'ä¼¤å¿ƒ', 'å¤±æœ›', 'çƒ¦', 'æ¨', 'æ€•']
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        pos_score = sum(1 for word in positive_words if word in text)
        neg_score = sum(1 for word in negative_words if word in text)
        
        total = pos_score + neg_score
        if total == 0:
            sentiment = "ä¸­æ€§"
            score = 0
        else:
            score = (pos_score - neg_score) / total
            if score > 0.3:
                sentiment = "ç§¯æ ğŸ˜Š"
            elif score < -0.3:
                sentiment = "æ¶ˆæ ğŸ˜”"
            else:
                sentiment = "ä¸­æ€§ ğŸ˜"
        
        msg = "ğŸ’­ **æƒ…æ„Ÿåˆ†æç»“æœ**\n"
        msg += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        msg += f"æ–‡æœ¬: {text[:50]}...\n" if len(text) > 50 else f"æ–‡æœ¬: {text}\n"
        msg += f"æƒ…æ„Ÿå€¾å‘: {sentiment}\n"
        msg += f"ç§¯æè¯æ•°: {pos_score}\n"
        msg += f"æ¶ˆæè¯æ•°: {neg_score}\n"
        msg += f"æƒ…æ„Ÿå¾—åˆ†: {score:.2f}\n"
        
        return {"reply": msg}


# å¯¼å‡ºæ‰€æœ‰æ’ä»¶
__all__ = ['DataAnalysisPlugin', 'WebScraperPlugin', 'NLPPlugin']